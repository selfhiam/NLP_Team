# **1. NLP기본개념**

### 1-1. 토큰(Token)
- 가장 기본이 되는 단어
- 토큰의 단위는 자연어 내에서 의미를 가지는 최소 단위로 정의
### 1-2. 토크나이징(Tokenizing)
- 토큰화 작업은 주어진 코퍼스 내 자연어 문장들을 토큰(최소 단위)으로 나누는 작업

> 코퍼스는 자연어 처리 연구나 애플리케이션 활용을 염두에 두고 수집된 텍스트 데이터셋을 의미, 쉽게 말해서 말뭉치

- 문장을 의미가 있는 가장 작은 단어들로 나누어, 나눠진 단어들을 이용해서 의미 분석
- 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야 하는 기본적인 작업
- 토크나이징을 어떻게 하느냐에 따라 성능 차이
- 사이버 보안, NFT 생성에 사용되는 것으로 유명
- 자연어 프로세스의 중요한 부분 차지

### 1-3. 토큰화 방법
- 문장 토큰화: 토큰의 기준을 문장으로 하는 토큰화 방법
    - 문장의 끝에 오는 문장 부호를 기준으로 코퍼스를 잘라냄(. 또는 ! 또는 ?)
    - 예) 빵상! 인간들아. 라는 뜻이란다.
    - 예외) aaa@naver.com

- 단어 토큰화(Word Tokenization): 토큰의 기준을 단어로 하는 토큰화 방법
    - 보편적으로 구분기호를 가지고 텍스트를 나누게 되며, 기본적으로 공백을 구분자로 사용
    - 한국어의 경우 교착어이기 때문에 공백으로 단어 토큰화 시 성능 하락
    - 새로운 단어 추가될수록 단어 사전의 크기가 계속 증가
    - OOV(Out of Vocabulary) 문제

- 문자 토큰화(Text Tokenization): 토큰의 기준을 문자로 하는 토큰화 방법
    - 단어 토큰화의 한계점들을 해결하기 위한 방법
    - 영어는 26개의 알파벳에 따라 분리, 한글은 자음 19개와 모음 21개의 글자에 따라 분리
    - 문장 하나를 생성하는데 너무 많은 추론이 필요
    - 단어 사전은 작지만 모델의 예측 시간에 문제가 생길 수 있음


> BPE와 WordPiece는 단어를 부분 단어(Subword) 라는 더 작은 형태로 나누는 토큰화 알고리즘



# **2. BPE**

- 코퍼스 내 단어의 등장 빈도에 따라 서브워드를 구축하는데 사용
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) 2016년 논문에서 처음 제안
- 글자단위에서 점진적으로 서브워드 집합을 만들어내는  Bottom-up 방식의 접근 방식으로 자연어 코퍼스에 있는 모든 단어들을 글자 단위로 분리, 등장 빈로에 따라 글자들을 서브워드로 통합하는 방식



1. 동작 방식:
   - BPE는 글자 또는 서브워드를 언어의 기본 단위로 간주.
   - 초기에는 각 단어를 글자 단위로 분리하여 토큰화.

2. 합치기 단계:
   - 가장 빈도가 높은 **글자 쌍**을 찾아 합침. 이 과정을 미리 정의된 횟수나 빈도에 도달할 때까지 반복
   - 예를 들어, "abracadabra"에서 가장 빈도가 높은 글자 쌍은 'a'와 'b'로, 이를 'ab'로 합침
   - 이렇게 합쳐진 새로운 글자 'ab'는 이제 다른 글자들과도 새로운 빈도를 가지게 됨

3. 서브워드 생성:
   - 위의 과정을 반복하여 새로운 글자 쌍을 합침
   - 합치는 과정을 통해 점진적으로 더 큰 서브워드가 생성되며, 빈도에 따라 단어들이 서브워드로 통합

4. 최종 결과:
   - 미리 정의된 횟수나 빈도에 도달하면 합치기를 중지하고 최종 서브워드 집합을 얻음.
   - 언어 모델이나 기계 번역 모델에서 이 서브워드 집합을 활용하여 효과적으로 단어를 표현하고 처리할 수 있음

> BPE는 희귀한 단어나 어떤 언어에 특화된 용어들에 대한 유연하고 효과적인 대처가 가능하며, 특히 데이터의 특성에 덜 민감한 점이 장점으로 평가됨





# **3. Word Piece**
- [Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144) 구글이 2016년도 논문에 처음 공개한 BPE의 변형 알고리즘
- "Word Piece"는 기본적으로 BPE(Byte Pair Encoding)와 유사한 방식으로 작동하지만, BPE가 자주 발생하는 쌍만을 병합하는 것과는 달리, "Word Piece"는 언어 모델 perplexity 측면에서의 관점을 중시
- 병합할 두 문자가 있을 때 각각의 문자가 따로 있을 때를 더 중요시 여기는지, 병합되었을 때를 더 중요시 여기는지에 차이점을 둠
- GPT 모델과 같은 생성 모델의 경우에는 BPE 알고리즘을 사용
- BERT, ELECTRA와 같은 자연어 이해 모델에서는 WordPiece Tokenizer 사용





1. 동작 방식:
   - Word Piece는 글자 또는 서브워드를 언어의 기본 단위로 삼음
   - 초기에는 각 단어를 글자 단위로 분리하여 토큰화

2. 합치기 단계:
- 언어 모델의 학습을 통해 점수가 높은 글자 쌍을 찾아 합침
- 등장 빈도가 아니라 언어 모델이 더 나은 예측을 할 수 있는 쌍을 선택하여 반복적으로 합침

3. 서브워드 생성:
   - 위의 과정을 반복하여 새로운 서브워드 쌍을 합침
   - 합치는 과정을 통해 점진적으로 더 큰 서브워드가 생성되며, 빈도에 따라 단어들이 서브워드로 통합

4. 최종 결과:
   - 미리 정의된 횟수나 빈도에 도달하면 합치기를 중지하고 최종 서브워드 집합을 얻음
   - Word Piece는 언어 모델이나 기계 번역 모델에서 이 서브워드 집합을 활용하여 효과적으로 단어를 표현하고 처리 가능





# **4. BPE와 Word Piece의 차이**
- Word Piece는 BPE보다 언어학적으로 더 세부적인 단위를 생성.
- 사전 훈련된 트랜스포머 모델에서 Unigram Language Model이 더 효과적으로 작동하는 것에 대한 일부 증거가 있으며, 다른 연구는 여러 토크나이제이션을 거쳐 앙상블을 수행

1. **BPE vs. Word Piece:**
   - BPE는 더 작은 단위로 분할하면서 언어적으로 덜 타당한 단위를 생성한다는 것을 나타냄. Word Piece가 이런 면에서 더 언어학적으로 타당한 결과를 도출한다고 이해할 수 있음.

2. **Unigram Language Model in Pre-trained Transformer Models:**
   - 이 문장에서는 사전 훈련된 트랜스포머 모델에서 Unigram Language Model이 더 나은 결과를 보인다는 증거에 언급. 이는 어휘 크기 축소 및 언어 모델링에서 Unigram 기반 접근이 효과적일 수 있다는 것을 시사함.

3. **Ensembling Across Multiple Tokenization:**
   - 다른 연구에서는 여러 토크나이제이션을 거쳐 앙상블을 수행하는 방법을 탐구하고 있는 것으로 보임. 이는 다양한 토큰화 방법을 결합하여 모델의 성능을 향상시키는 실험을 수행하고 있음을 나타냄.




